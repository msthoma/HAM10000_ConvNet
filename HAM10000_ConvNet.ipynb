{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAM10000 ConvNet",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LregSzqlusEa",
        "colab_type": "text"
      },
      "source": [
        "# HAM10000 ConvNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60yc0TKmX2WP",
        "colab_type": "text"
      },
      "source": [
        "Convolutional neural network for identifying skin lesions, using the [HAM10000 dataset](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T). \n",
        "\n",
        "I trained the ConvNet using [Google Colaboratory](https://colab.research.google.com/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdxPpC2utyA5",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rwLaYEiu1gr",
        "colab_type": "text"
      },
      "source": [
        "Import/install required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw6wr1TvuMqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%matplotlib inline \n",
        "\n",
        "import io\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.python.keras.models import Sequential, Input, Model\n",
        "\n",
        "# other important info\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "dx_ints = {\"akiec\": 0, \"bcc\": 1, \"bkl\": 2,\n",
        "           \"df\": 3, \"nv\": 4, \"vasc\": 5, \"mel\": 6}\n",
        "dx_list = [\"akiec\", \"bcc\", \"bkl\", \"df\", \"nv\", \"vasc\", \"mel\"]\n",
        "num_classes = len(dx_list)\n",
        "base_dir = \"/content/drive/My Drive/Colab Notebooks/HAM_ConvNet\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-vXMs0VwL13",
        "colab_type": "text"
      },
      "source": [
        "Mounts Google Drive, where I stored the input data sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD-PIgsWs5Ca",
        "colab_type": "code",
        "outputId": "2784cb82-8196-4c08-cfb2-eefef5977124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxJvPgJTuAiG",
        "colab_type": "text"
      },
      "source": [
        "Function to pre-process data, splits them into train, validation, and test subsets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUD0ZHmCt_fE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data(file_name):\n",
        "  # read data\n",
        "  dataset = pd.read_csv(f\"{base_dir}/{file_name}\", header=0)\n",
        "  \n",
        "  # infer image resolution (assumes square images!)\n",
        "  img_dim = int(math.sqrt(len(dataset.columns)))\n",
        "\n",
        "  # split data to images and labels\n",
        "  data_x = np.array([np.reshape(row.to_numpy(), (-1, img_dim)) for _, \n",
        "                     row in dataset.iloc[:,:-1].iterrows()])\n",
        "  data_y = dataset.loc[:,\"label\"].to_numpy()\n",
        "\n",
        "  # split to training and testing subsets\n",
        "  train_X, test_X, train_Y, test_Y = train_test_split(data_x, data_y, \n",
        "                                                      test_size=0.2, \n",
        "                                                      random_state=624)\n",
        "\n",
        "  # reshape data to 2D arrays\n",
        "  train_X = train_X.reshape(-1, img_dim, img_dim, 1)\n",
        "  test_X = test_X.reshape(-1, img_dim, img_dim, 1)\n",
        "\n",
        "  # convert grayscale values to 0-1 scale\n",
        "  train_X = train_X.astype('float32')\n",
        "  test_X = test_X.astype('float32')\n",
        "  train_X = train_X / 255.\n",
        "  test_X = test_X / 255.\n",
        "\n",
        "  # change the labels from categorical to one-hot encoding\n",
        "  train_Y_one_hot = to_categorical(train_Y)\n",
        "  test_Y_one_hot = to_categorical(test_Y)\n",
        "\n",
        "  # split train data again to create validation subset\n",
        "  train_X,valid_X,train_label,valid_label = train_test_split(\n",
        "      train_X, train_Y_one_hot, test_size=0.2, random_state=624)\n",
        "  \n",
        "  # print(train_X.shape, test_X.shape)\n",
        "  # print(train_Y.shape, test_Y.shape)\n",
        "  # print('Original label:', train_Y[0])\n",
        "  # print('After conversion to one-hot:', train_Y_one_hot[0])\n",
        "  \n",
        "  # package results in dictionary\n",
        "  res = {\"trX\": train_X,\"valX\": valid_X,\n",
        "         \"trLab\": train_label, \"valLab\": valid_label, \n",
        "         \"tesX\": test_X, \"tesYhot\": test_Y_one_hot, \"tesY\": test_Y,\n",
        "         \"img_dim\": img_dim}\n",
        "  \n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcZFfYqCudZC",
        "colab_type": "text"
      },
      "source": [
        "Function to create and compile the ConvNet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8cGgheyugew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_conv_net_model(img_dim):\n",
        "\n",
        "  # set up convNet model\n",
        "  # configuration of the model from this tutorial:\n",
        "  # https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\n",
        "  ham_model = Sequential()\n",
        "  ham_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',\n",
        "                       padding='same',input_shape=(img_dim,img_dim,1)))\n",
        "  ham_model.add(LeakyReLU(alpha=0.1))\n",
        "  ham_model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "  ham_model.add(Dropout(0.25))\n",
        "  ham_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
        "  ham_model.add(LeakyReLU(alpha=0.1))\n",
        "  ham_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "  ham_model.add(Dropout(0.25))\n",
        "  ham_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
        "  ham_model.add(LeakyReLU(alpha=0.1))                  \n",
        "  ham_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "  ham_model.add(Dropout(0.4))\n",
        "  ham_model.add(Flatten())\n",
        "  ham_model.add(Dense(128, activation='linear'))\n",
        "  ham_model.add(LeakyReLU(alpha=0.1))           \n",
        "  ham_model.add(Dropout(0.3))\n",
        "  ham_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "  # compile model\n",
        "  ham_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                    optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
        "  \n",
        "  return ham_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIuq-dKIup9P",
        "colab_type": "text"
      },
      "source": [
        "Function that trains and evaluates the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FdjWc-P89aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_conv_net(data_file_name, batch_size=batch_size, epochs=epochs, \n",
        "                   verbose=1):\n",
        "  # get data\n",
        "  data = preprocess_data(data_file_name)\n",
        "\n",
        "  # create model\n",
        "  model = create_conv_net_model(data[\"img_dim\"])\n",
        "\n",
        "  # train model\n",
        "  model_train_info = model.fit(data[\"trX\"], data[\"trLab\"],batch_size,epochs,\n",
        "                                verbose,validation_data=(data[\"valX\"],\n",
        "                                                         data[\"valLab\"]))\n",
        "  \n",
        "  # save trained model\n",
        "  model_dir_name = f\"{data_file_name.strip('.csv')}_model\"\n",
        "  model.save(f\"{base_dir}/HAM_models/{model_dir_name}\")\n",
        "\n",
        "  # evaluate\n",
        "  test_eval = model.evaluate(data[\"tesX\"], data[\"tesYhot\"])\n",
        "  print('Test loss:', test_eval[0])\n",
        "  print('Test accuracy:', test_eval[1])\n",
        "\n",
        "  # get training info\n",
        "  accuracy = model_train_info.history['accuracy']\n",
        "  val_accuracy = model_train_info.history['val_accuracy']\n",
        "  loss = model_train_info.history['loss']\n",
        "  val_loss = model_train_info.history['val_loss']\n",
        "\n",
        "  # make predictions\n",
        "  predicted_classes = model.predict(data[\"tesX\"])\n",
        "  predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "  # create classification report\n",
        "  target_names = dx_list  # [f\"Class {dx}\" for dx in dx_list]\n",
        "  report = classification_report(data[\"tesY\"], predicted_classes, \n",
        "                        target_names=target_names, output_dict=True)\n",
        "  report = pd.DataFrame(report)\n",
        "\n",
        "  # create confusion matrix\n",
        "  cm = confusion_matrix(data[\"tesY\"], predicted_classes)\n",
        "  df_cm = pd.DataFrame(cm, dx_list, dx_list)\n",
        "\n",
        "  # package results \n",
        "  res = {\"test_eval\": test_eval, \"accuracy\": accuracy, \n",
        "         \"val_accuracy\": val_accuracy, \"loss\": loss, \"val_loss\": val_loss,\n",
        "         \"predicted_classes\": predicted_classes, \"test_Y\": data[\"tesY\"],\n",
        "         \"confusion_matrix\": df_cm, \"report\": report, \"model\": model}\n",
        "  \n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkSIgxNcqbVJ",
        "colab_type": "text"
      },
      "source": [
        "## Training networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTeHxk5rvkS9",
        "colab_type": "text"
      },
      "source": [
        "I trained the 5 ConvNets, using 5 different data sets:\n",
        "\n",
        "1.   Data set with 8x8 pixel images (as provided in the HAM data set).\n",
        "2.   Data set with 28x28 images (as provided in the HAM data set).\n",
        "3.   Data set with 28x28 images, same as above, but with augmented lesion types to 2000 images per category. \n",
        "4.   Data set with 42x42 images, that I created, with augmented lesion types to 2000 per category. \n",
        "5.   Data set with 64x64 images, that I created, with augmented lesion types to 2000 per category.\n",
        "\n",
        "The networks were all trained for 30 epochs. The cells below show information about the training of these networks, as well as their evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJHfnWMbq33o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96312a6b-9464-4c19-b5f0-8ec04df89630"
      },
      "source": [
        "net8 = train_conv_net(\"hmnist_8_8_L.csv\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "101/101 [==============================] - 2s 17ms/step - loss: 1.2130 - accuracy: 0.6544 - val_loss: 1.1917 - val_accuracy: 0.6862\n",
            "Epoch 2/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 1.1521 - accuracy: 0.6634 - val_loss: 1.0782 - val_accuracy: 0.6862\n",
            "Epoch 3/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 1.1223 - accuracy: 0.6636 - val_loss: 1.0401 - val_accuracy: 0.6862\n",
            "Epoch 4/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 1.0775 - accuracy: 0.6634 - val_loss: 1.0063 - val_accuracy: 0.6862\n",
            "Epoch 5/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 1.0638 - accuracy: 0.6625 - val_loss: 1.0050 - val_accuracy: 0.6862\n",
            "Epoch 6/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 1.0511 - accuracy: 0.6623 - val_loss: 1.0382 - val_accuracy: 0.6881\n",
            "Epoch 7/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 1.0395 - accuracy: 0.6627 - val_loss: 0.9935 - val_accuracy: 0.6868\n",
            "Epoch 8/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 1.0202 - accuracy: 0.6628 - val_loss: 0.9671 - val_accuracy: 0.6881\n",
            "Epoch 9/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 1.0126 - accuracy: 0.6667 - val_loss: 0.9557 - val_accuracy: 0.6875\n",
            "Epoch 10/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 1.0145 - accuracy: 0.6684 - val_loss: 0.9666 - val_accuracy: 0.6881\n",
            "Epoch 11/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 1.0019 - accuracy: 0.6689 - val_loss: 0.9509 - val_accuracy: 0.6962\n",
            "Epoch 12/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9837 - accuracy: 0.6691 - val_loss: 0.9370 - val_accuracy: 0.6893\n",
            "Epoch 13/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9850 - accuracy: 0.6695 - val_loss: 0.9526 - val_accuracy: 0.6981\n",
            "Epoch 14/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9729 - accuracy: 0.6720 - val_loss: 0.9355 - val_accuracy: 0.6968\n",
            "Epoch 15/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9619 - accuracy: 0.6731 - val_loss: 0.9327 - val_accuracy: 0.6968\n",
            "Epoch 16/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9612 - accuracy: 0.6755 - val_loss: 0.9072 - val_accuracy: 0.6999\n",
            "Epoch 17/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9580 - accuracy: 0.6786 - val_loss: 0.9235 - val_accuracy: 0.7018\n",
            "Epoch 18/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9454 - accuracy: 0.6761 - val_loss: 0.9063 - val_accuracy: 0.7006\n",
            "Epoch 19/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9451 - accuracy: 0.6798 - val_loss: 0.8932 - val_accuracy: 0.7049\n",
            "Epoch 20/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9293 - accuracy: 0.6820 - val_loss: 0.8911 - val_accuracy: 0.6974\n",
            "Epoch 21/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9372 - accuracy: 0.6822 - val_loss: 0.9367 - val_accuracy: 0.7056\n",
            "Epoch 22/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9352 - accuracy: 0.6853 - val_loss: 0.8981 - val_accuracy: 0.7012\n",
            "Epoch 23/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.9229 - accuracy: 0.6804 - val_loss: 0.9076 - val_accuracy: 0.7105\n",
            "Epoch 24/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.9254 - accuracy: 0.6886 - val_loss: 0.8959 - val_accuracy: 0.7062\n",
            "Epoch 25/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.9219 - accuracy: 0.6875 - val_loss: 0.8888 - val_accuracy: 0.7105\n",
            "Epoch 26/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.9105 - accuracy: 0.6864 - val_loss: 0.8783 - val_accuracy: 0.7043\n",
            "Epoch 27/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.9082 - accuracy: 0.6889 - val_loss: 0.8905 - val_accuracy: 0.7062\n",
            "Epoch 28/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.9107 - accuracy: 0.6901 - val_loss: 0.8982 - val_accuracy: 0.7031\n",
            "Epoch 29/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.9146 - accuracy: 0.6918 - val_loss: 0.8806 - val_accuracy: 0.7105\n",
            "Epoch 30/30\n",
            "101/101 [==============================] - 2s 15ms/step - loss: 0.9035 - accuracy: 0.6906 - val_loss: 0.8813 - val_accuracy: 0.7012\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/HAM_ConvNet/HAM_models/hmnist_8_8_L_model/assets\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.8783 - accuracy: 0.6960\n",
            "Test loss: 0.8783152103424072\n",
            "Test accuracy: 0.6959560513496399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tjazATJjtvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net28 = train_conv_net(\"hmnist_28_28_L.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV3xQv9fqX6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net28_aug = train_conv_net(\"hmnist_28_28_2000.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBnh4OJBqskl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net42_aug = train_conv_net(\"hmnist_42_42_2000.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mUiQ3Q_qyTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net64_aug = train_conv_net(\"hmnist_64_64_2000.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwEQIuRWqg-W",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvRmz50F1rz4",
        "colab_type": "text"
      },
      "source": [
        "The results from the training of networks are presented in this section. \n",
        "\n",
        "Below I present the results in a series of graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj65VaH27Cy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = [net8, net28, net28_aug, net42_aug, net64_aug]\n",
        "\n",
        "titles = [\"8x8 (as provided)\", \"28x28 (as provided)\", \"28x28 (augmented)\", \n",
        "          \"42x42 (augmented)\", \"64x64 (augmented)\"]\n",
        "\n",
        "# Find colorbar max for confusion matrices\n",
        "cbar_max = max(max(d[\"confusion_matrix\"].max(axis=1)) for d in all_data)\n",
        "\n",
        "# create figure\n",
        "fig, axes = plt.subplots(4, 5, sharey=\"row\", figsize=(18,14),\n",
        "                         num=\"default\", squeeze=True)\n",
        "epch = range(epochs)\n",
        "\n",
        "# add graphs to figure\n",
        "for i, r in enumerate(all_data):\n",
        "\n",
        "  axes[0, i].set_title(titles[i], fontsize=14)\n",
        "\n",
        "  # Training and validation accuracy graphs\n",
        "  if i == 0: axes[0, i].set_ylabel(\"Training and validation accuracy\",\n",
        "                                   fontsize=13)\n",
        "  axes[0, i].set_xlabel(\"Epochs\", fontsize=10)\n",
        "  axes[0, i].plot(epch, r[\"accuracy\"], \"bo\", label='Training accuracy')\n",
        "  axes[0, i].plot(epch, r[\"val_accuracy\"], \"b\", label='Validation accuracy')\n",
        "  axes[0, i].legend(fontsize=11)\n",
        "  axes[0, i].tick_params(axis='both', which='major', labelsize=11)\n",
        "\n",
        "  # Training and validation loss graphs\n",
        "  if i == 0: axes[1, i].set_ylabel(\"Training and validation loss\", fontsize=13)\n",
        "  axes[1, i].set_xlabel(\"Epochs\", fontsize=10)\n",
        "  axes[1, i].plot(epch, r[\"loss\"], \"bo\",  label='Training loss', color =\"red\")  \n",
        "  axes[1, i].plot(epch, r[\"val_loss\"], \"b\", label='Validation loss',\n",
        "                  color =\"red\")\n",
        "  axes[1, i].legend(fontsize=11)\n",
        "  axes[1, i].tick_params(axis='both', which='major', labelsize=11)\n",
        "\n",
        "  # Prediction precision graphs\n",
        "  df = r[\"report\"].T\n",
        "  bar_data = df.loc[\"akiec\":\"mel\",\"precision\"]\n",
        "  if i == 0: axes[2, i].set_ylabel(\"Prediction precision\", fontsize=13)\n",
        "  axes[2, i].bar(bar_data.index, bar_data, color=\"rgbcmyk\")\n",
        "  axes[2, i].tick_params(axis='both', which='major', labelsize=11)\n",
        "\n",
        "  # Confusion matrices  \n",
        "  cm = confusion_matrix(r[\"test_Y\"], r[\"predicted_classes\"])\n",
        "  df_cm = pd.DataFrame(cm, dx_list, dx_list)\n",
        "  sn.set(font_scale=.7)\n",
        "  sn.heatmap(df_cm, ax=axes[3, i], vmin=0, vmax=cbar_max, annot=True, \n",
        "             cmap=\"YlGnBu\", cbar=i==len(all_data)-1, fmt=\"d\", \n",
        "             linewidths=0.1, linecolor=\"black\")\n",
        "  if i == 0: axes[3, i].set_ylabel(\"Confusion matrices\", fontsize=13)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"figure_convNetRes.svg\", bbox_inches=\"tight\")\n",
        "fig.savefig(\"figure_convNetRes.pdf\", bbox_inches=\"tight\")\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3taXmH34FqoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create results table\n",
        "import datetime\n",
        "evaluation_data = pd.DataFrame([d[\"test_eval\"] for d in all_data], index=titles,\n",
        "                               columns=[\"Test loss\", \"Test accuracy\"])\n",
        "times = [67, 508, 1313, 3163, 5906]\n",
        "evaluation_data[\"Training time\"] = [str(datetime.timedelta(seconds=i)) for i \n",
        "                                    in times]\n",
        "evaluation_data[\"Epochs\"] = [epochs]*5\n",
        "\n",
        "# re-arrange columns \n",
        "evaluation_data = evaluation_data.iloc[:,[3,2,0,1]]\n",
        "# save to file\n",
        "evaluation_data.to_csv(\"table_results.csv\")\n",
        "\n",
        "evaluation_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S2UJl1wK8Jr",
        "colab_type": "code",
        "outputId": "c7f68159-db5b-4fff-bd01-991593d36eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Convert models\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(net28_aug[\"model\"])\n",
        "tflite_model = converter.convert()\n",
        "# tflite_model.save(f\"{base_dir}/HAM_models/tflite_28\")\n",
        "print(type(tflite_model))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'bytes'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DL4bNd7Poyg",
        "colab_type": "code",
        "outputId": "3027b145-0286-4678-8050-a13ffbe968ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(tflite_model))\n",
        "with open(f\"{base_dir}/HAM_models/tflite_28\", \"wb\") as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1426968\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}